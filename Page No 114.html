<html>

<head>

<title> Data Mining : page no 114 </title>

<style type="text/css">
p{
  text-indent:30px;
  text-align:justify;
}
body{
   margin-left:450px;
}

</style>

</head>


<body>



<p><i>Table 4: Experimental results of MEE/ANN</i></p>

<table border=" 1" width = "500" heigth = "300" >


<tr>
<th rowspan="2">Date sets</th>
<th Colspan="2">single net</th>
<th rowspan="2">Bagging</th>
<th rowspan="2">AdaBoost</th>
<th rowspan="2">GEFS</th>
<th Colspan="3">MEE</th>
</tr>

<tr>
<td>Avg</td>
<td>S.D</td>
<td>Avg</td>
<td>S.D</td>
<td>Epochs</td>
</tr>

<tr>
<td>Credita</td>
<td>84.3</td>
<td>0.30</td>
<td>84.2</td>
<td>86.3</td>
<td>86.8</td>
<td>86.4</td>
<td>0.52</td>
<td>40</td>
</tr>  

<tr>
<td>Creditg</td>
<td>71.7</td>
<td>0.43</td>
<td>75.8</td>
<td>74.7</td>
<td>75.2</td>
<td>75.6</td>
<td>0.42</td>
<td>50</td>
</tr>

<tr>
<td>diabetes</td>
<td>76.4</td>
<td>0.93</td>
<td>77.2</td>
<td>76.7</td>
<td>77.0</td>
<td>76.8</td>
<td>0.42</td>
<td>50</td>
</tr>

<tr>
<td>Glass</td>
<td>57.1</td>
<td>2.69</td>
<td>66.9</td>
<td>68.9</td>
<td>69.6</td>
<td>67.1</td>
<td>1.73</td>
<td>100</td>
</tr>

<tr>
<td>Cleveland</td>
<td>80.7 </td>
<td>1.83</td>
<td>83.0</td>
<td>78.9</td>
<td>83.9</td>
<td>83.3</td>
<td>1.54 </td>
<td>50 </td>
</tr>

<tr>
<td>Hepatitis</td>
<td>81.5</td>
<td>0.21</td>
<td>82.2</td>
<td>80.3</td>
<td>83.3</td>
<td> 84.9 </td>
<td>0.65</td>
<td>40</td>
</tr>

<tr>
<td>Votes-84</td>
<td>95.9 </td>
<td>0.41</td>
<td>95.9</td>
<td>94.7</td>
<td>95.6</td>
<td>96.1 </td>
<td>0.44 </td>
<td>40 </td>
</tr>

<tr>
<td>Hypo</td>
<td>93.8</td>
<td>0.09</td>
<td>93.8 </td>
<td>93.8 </td>
<td>94.1</td>
<td>93.9</td>
<td> 0.06 </td>
<td>50</td>
</tr>


<tr>
<td>Ionosphere </td>
<td> 89.3</td>
<td>0.85 </td>
<td>90.8 </td>
<td>91.7 </td>
<td>94.6 </td>
<td>93.5</td>
<td>0.81 </td>
<td>100 </td>
</tr>

<tr>
<td>Iris </td>
<td>95.9</td>
<td>1.10 </td>
<td>96.0 </td>
<td>96.1 </td>
<td> 96.7</td>
<td>96.5 </td>
<td> 0.73</td>
<td>100 </td>
</tr>


<tr>
<td>Krvskp</td>
<td>98.8</td>
<td>0.63 </td>
<td>99.2</td>
<td>99.7 </td>
<td>99.3 </td>
<td>99.3</td>
<td>0.10</td>
<td>50 </td>
</tr>

<tr>
<td>Labor</td>
<td>91.6</td>
<td>2.29</td>
<td>95.8</td>
<td>96.8</td>
<td>96.5 </td>
<td>94.4</td>
<td>0.78 </td>
<td>50 </td>
</tr>

<tr>
<td>Segment</td>
<td>92.3</td>
<td>0.97 </td>
<td>94.6 </td>
<td>96.7</td>
<td>96.4</td>
<td>93.2</td>
<td>0.28 </td>
<td> 50 </td>
</tr>

<tr>
<td>Sick</td>
<td>95.2</td>
<td> 0.47 </td>
<td>94.3</td>
<td>95.5 </td>
<td>96.5</td>
<td>99.3</td>
<td>0.03 </td>
<td >50 </td>
</tr>

<tr>
<td>Sonar</td>
<td>80.5</td>
<td> 2.03 </td>
<td>83.2</td>
<td>87.0</td>
<td>82.2</td>
<td>85.2</td>
<td>1.57 </td>
<td>100</td>
</tr>


<tr>
<td>Soybean</td>
<td>92.0</td>
<td>0.92</td>
<td>93.1</td>
<td>93.7</td>
<td>94.1</td>
<td>93.8 </td>
<td>0.19 </td>
<td>50 </td>
</tr>

<tr>
<td>Vehicle</td>
<td>74.7</td>
<td>0.48</td>
<td>79.3</td>
<td>80.3</td>
<td>81.0</td>
<td>76.4</td>
<td>1.12 </td>
<td>50</td>
</tr>



<tr>
<td>Win-loss-tie </td>
<td colspan="2">15-0-2 </td>
<td>7-4-6</td>
<td>9-6-2</td>
<td>4-7-6</td>
<td colspan="3"> </td>
</tr>



</table>

<p>fold cross-validation experiments. Within the training algorithm, each ANN is trained on<br/>
two-thirds of the training set and tested on the remaining third for energy allocation<br/>
purposes. We present the performance of a single neural network using the complete<br/>
set of features as a baseline algorithm. In the win-loss-tie results shown at the bottom<br/>
of Table 4, a comparison is considered a tie if the intervals defined by one standard error8<br/>
of the mean overlap. Of the data sets tested, MEE shows consistent improvement over<br/>
a single neural network.</P>

<p>We also include the results of Bagging, AdaBoost, and GEFS from Opitz (1999) for<br/>
indirect comparison. In these comparisons, we did not have access to the accuracy<br/>
results of the individual runs. Therefore, a tie is conservatively defined as a test in which<br/>
the one standard-deviation interval of our test contained the point estimate of accuracy<br/>
from Opitz(1999). In terms of predictive accuracy, our algorithm demonstrates better or<br/>
equal performance compared to single neural networks, Bagging and Boosting. However,<br/>
MEE shows slightly worse performance compared to GEFS, possibly due to the method<br/>
-ological differences.For example, it is possible that the more complex structure of neural<br/>
networks used in GEFS can learn more difficult patterns in data sets such as Glass and<br/>
Labor data.</p>

<p>From the perspective of computational complexity, our algorithm can be very slow<br/>
compared to Bagging and Boosting. However, MEE can be very fast compared to GEFS,<br/>
because GEFS uses twice as many as input features as MEE. Further, the larger number<br/>
of hidden nodes and longer training epochs can make GEFS extremely slow.</p>







</body>
</html>